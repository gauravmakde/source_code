CREATE OR REPLACE TEMPORARY VIEW csv_test USING CSV 
OPTIONS(path "s3://{s3_bucket_root_var}/test.csv",
  sep  ",",
  inferSchema  "true",
  header "true");

--- s3_bucket_root_var:
    -- acedev-etl in dev
    -- ace-etl in prod

-- CSV files have several read options:
-- sep: file separator
-- inferSchema: Spark will inferSchema, else false will read all as string
-- header: tells Spark that this file has a header

--- CSV files have several options for writing - we can specify them in the YML arguments:

-- (in nonprod.yml / dev environment):
-- S3_CSV_EXAMPLE:
--   <<: *s3_csv
--   path: "s3://acedev-etl/Isf_test/ho6s/s3_csv_out/"
--   header: "true"
--   compression: "none"


-- sep (default ,): sets a single character (can provide unicode values) as a separator for each field and value.
-- header (default false): writes the names of columns as the first line.
-- compression (default snappy): compression codec to use when saving to file. 
-- This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate).


--- we can also have several READ options too! we can specify schema manually - or use the above inferSchema
CREATE OR REPLACE TEMPORARY VIEW csv_test_with_schema (a STRING, b STRING, c INT)  USING CSV 
OPTIONS(path "s3://acedev-etl/test.csv",
  sep  ",",
  header "true");


--- we can also provide gz/csv compression options too:
CREATE OR REPLACE TEMPORARY VIEW csv_test_with_compression (a STRING, b STRING, c INT)  USING CSV 
OPTIONS(path "s3://acedev-etl/test.csv.gz",
  sep  ",",
  compression "gzip",
  header "true");


-- final path is always path from conn_name + sql_table_reference:
-- in dev it would output like:
-- "s3://acedev-etl/Isf_test/ho6s/s3_csv_out/s3_csv/<auto-generated CSV name>.csv"

insert overwrite table s3_csv
select * from csv_test;

-- we can additionally read CSV files in ISF using paths too:
CREATE OR REPLACE TEMPORARY VIEW csv_test_2 USING CSV 
OPTIONS(path "s3://acedev-etl/Isf_test/ho6s/s3_csv_out/s3_csv/",
  sep  ",",
  inferSchema  "true",
  header "true");

insert overwrite table s3_csv
select * from csv_test_2;

-- csv's can also utilize S3 partitions for folder structures:

-- insert overwrite table s3_csv partition(som_col)
-- select * from csv_test_2;