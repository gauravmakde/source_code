[Path]
#Add input file path and please add in dags folder only

Input_Folder_Path=C:/Users/Gaurav.Makde/PycharmProjects/pythonProject/Dataproc_Output_automation/**/dags/*.py
Output_directory=C:/Users/Gaurav.Makde/PycharmProjects/pythonProject/Dataproc_Output_automation/op/
# add the directory where you want to copy the sql file in GCS bucket
testing_sql_directory=gs://test-data-datametica/app09392/isf-artifacts/sql
# add the directory where you want to copy the json file in GCS bucket
testing_json_directory=gs://test-data-datametica/app09392/isf-artifacts/user-config
# add the location of single file input but ut should start from rootfolder dags
Single_Csv_File=C:/Users/Gaurav.Makde/PycharmProjects/pythonProject/Dataproc_Output_automation


[folder]
Json_folder_name=pypeline_jobs
sql_folder_name=sql
kafka_json_folder=user-config
user_config_folder=env_configs
#Input_directory should be added as it will split and give the location ex: C:/Users/Gaurav.Makde/PycharmProjects/pythonProject/Dataproc_Atomation/ip/dags/gaurav to dags/gaurav
Input_directory=ip
Output_directory=op
# add the csv file for any specific adoc dags
csv_file_name=Input_specific_Dag.csv
new_relic=metrics

[Utility]
# for json and sql metadata
Enable_to_run_specific_file=N
# for yaml metadata
Fetch_metadata_newrelic_yaml=N
#for output code in path in which format it required
os_type=mac



